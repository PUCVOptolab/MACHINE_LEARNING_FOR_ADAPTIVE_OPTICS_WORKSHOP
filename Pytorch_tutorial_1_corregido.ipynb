{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pytorch_logo.png](data:image/png;base64,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)\n",
    "# Machine Learning for Adaptive Optics Workshop\n",
    "\n",
    "Benjamín González Barraza\n",
    "\n",
    "03-10-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/FOGuzman/carpetaMadre.git\n",
    "%cd carpetaMadre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch?\n",
    "\n",
    "PyTorch is an open-source deep learning framework designed to make building and training neural networks straightforward. It offers a flexible, Python interface that’s easy to debug and extend, and it natively taps into GPUs to speed up data processing and training. Thanks to a large community and a rich ecosystem of pretrained models and libraries, PyTorch is a go-to choice for deploying deep learning solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does Colab already include PyTorch?\n",
    "\n",
    "Yes — Google Colab ships with PyTorch preinstalled.  \n",
    "Run the cell below to confirm the version and check if a GPU is available.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does `torch.cuda.is_available()` check?\n",
    "\n",
    "`torch.cuda.is_available()` returns **True** if PyTorch detects a working CUDA setup and can use a GPU through its CUDA API.  \n",
    "If it returns **False**, PyTorch will run on the **CPU** instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU shows **False**, go to **Runtime → Change runtime type → Hardware accelerator → GPU** and rerun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2788e02",
   "metadata": {},
   "source": [
    "### Interpreting the version & CUDA info\n",
    "\n",
    "- `torch.__version__` — the **PyTorch package version** you installed.\n",
    "- `torch.version.cuda` — the **CUDA toolkit version PyTorch was built with** (compile-time).\n",
    "- `torch.cuda.get_device_name(0)` — the **actual GPU model** available at runtime (if any).\n",
    "\n",
    "These numbers can differ (e.g., PyTorch built with CUDA 12.1, running on a system with a newer driver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu129\n",
      "CUDA available: True\n",
      "CUDA toolkit (built with): 12.9\n",
      "Device: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('CUDA toolkit (built with):', torch.version.cuda)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Device:', torch.cuda.get_device_name(0) if device=='cuda' else 'CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors\n",
    "\n",
    "A **tensor** is a multidimensional array (scalars, vectors, matrices, and higher-rank arrays) and is the basic unit of computation in PyTorch.\n",
    "\n",
    "Tensors let you:\n",
    "- Represent numeric data with specific dtypes (e.g., `float32`, `float64`, `int64`).\n",
    "- Move data between **CPU** and **GPU** to accelerate operations.\n",
    "- Carry gradients (`requires_grad=True`) for training via autograd.\n",
    "- More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Python scalar vs. PyTorch tensor\n",
    "\n",
    "Below we compare an integer with a PyTorch tensor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor(1)\n",
    "B = 1\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring tensors\n",
    "\n",
    "There are many ways to create tensors in PyTorch.  \n",
    "We’ll start with `torch.zeros((a, b))`, which creates a 2D tensor (matrix) of shape `(a, b)` filled with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.zeros((2,3))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor attributes\n",
    "\n",
    "PyTorch tensors expose a set of attributes you’ll use constantly:\n",
    "\n",
    "- **`x.shape` / `x.size()`** — The tensor’s dimensions. `size()` and `shape` return the same info (e.g., `(a, b)` for a 2-D matrix).\n",
    "- **`x.ndim`** — The number of dimensions (tensor rank). For a matrix `(a, b)`, `ndim == 2`.\n",
    "- **`x.dtype`** — The data type (e.g., `torch.float32`, `torch.float64`, `torch.int64`).  \n",
    "  *Note:* only floating/complex dtypes can track gradients.\n",
    "- **`x.device`** — Where the tensor lives: `cpu` or `cuda:0` (etc.). Move with `x.to(device)`, `x.cpu()`, `x.cuda()`.\n",
    "- **`x.numel()`** — Total number of elements (same as `x.nelement()`).\n",
    "- **`x.requires_grad`** — If `True`, autograd will track ops on `x` and accumulate gradients into `x.grad` when you call `backward()`.\n",
    "- **`x.grad` / `x.grad_fn`** —  \n",
    "  - `x.grad` holds the gradient **after** `backward()` runs, but only for **leaf** tensors with `requires_grad=True`.  \n",
    "  - `x.grad_fn` is the function that produced the tensor; it’s **`None` for leaf tensors**, and set (e.g., `AddBackward0`) for results of operations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 3])\n",
      "2\n",
      "torch.float32\n",
      "cpu\n",
      "6\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(A.shape, A.size())\n",
    "print(A.ndim)\n",
    "print(A.dtype)\n",
    "print(A.device)\n",
    "print(A.numel())\n",
    "print(A.requires_grad)\n",
    "print(A.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations with tensors\n",
    "\n",
    "Most **NumPy-style** operations exist in PyTorch with similar semantics.  \n",
    "You can use **operators** (`+`, `-`, `*`, `/`, `@`) or **functional forms** (`torch.add`, `torch.mul`, `torch.matmul`, …).  \n",
    "Broadcasting, advanced indexing, reductions, and linear algebra follow familiar NumPy patterns—plus GPU and autograd support.\n",
    "\n",
    "**Common families**\n",
    "- **Elementwise & broadcasting:** `+ - * /`, `torch.add/mul/div`, `torch.exp/log/sqrt`, `torch.clamp`, …\n",
    "- **Reductions:** `x.sum(dim=...)`, `x.mean`, `x.max`, `x.argmax`, `x.std`, …\n",
    "- **Linear algebra:** `@`, `torch.matmul`, `torch.mm`, `torch.mv`, `torch.linalg.inv`, `torch.linalg.svd`, `torch.einsum`\n",
    "- **Reshaping:** `x.view`, `x.reshape`, `x.permute`, `x.transpose`, `x.squeeze/unsqueeze`, `x.flatten`\n",
    "- **Join/split:** `torch.cat`, `torch.stack`, `torch.split`, `torch.chunk`\n",
    "- **Indexing/masks:** slices, boolean masks, `gather`, `where`\n",
    "- **Type/device:** `x.to(dtype/device)`, `x.float()`, `x.cuda()`, `x.cpu()`\n",
    "- **In-place ops:** methods ending in `_` (e.g., `x.add_(1)`) — use with care when using autograd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.shape: torch.Size([2, 3]) | m.shape: torch.Size([2, 4])\n",
      "sum over rows: tensor([0.9945, 2.9490])\n",
      "mean all elements: tensor(0.6573)\n",
      "argmax per row: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Elementwise, broadcasting, reductions, matmul\n",
    "import torch\n",
    "\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.ones(1, 3)         # broadcasts over dim 0\n",
    "c = a * 2 + b                # elementwise + broadcasting\n",
    "m = a @ (torch.randn(3, 4))  # matrix multiply → (2, 4)\n",
    "\n",
    "print(\"c.shape:\", c.shape, \"| m.shape:\", m.shape)\n",
    "print(\"sum over rows:\", c.sum(dim=1))\n",
    "print(\"mean all elements:\", c.mean())\n",
    "print(\"argmax per row:\", c.argmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2: torch.Size([3, 4]) x3: torch.Size([4, 3]) flat: torch.Size([12])\n",
      "cat: torch.Size([6, 4]) stack: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Reshape / view / permute + concat/stack\n",
    "x = torch.arange(12.)                            # 1D: (12,)\n",
    "x2 = x.view(3, 4)                                # view if contiguous\n",
    "x3 = x2.permute(1, 0).contiguous()               # (4, 3), ensure contiguous if needed\n",
    "flat = x3.reshape(-1)                            # safe reshape to 1D\n",
    "\n",
    "cat = torch.cat([x2, x2], dim=0)                  # (6, 4)\n",
    "stk = torch.stack([x2, x2], dim=0)                # (2, 3, 4)\n",
    "\n",
    "print(\"x2:\", x2.shape, \"x3:\", x3.shape, \"flat:\", flat.shape)\n",
    "print(\"cat:\", cat.shape, \"stack:\", stk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positives count: 11\n",
      "M2 nonneg min/max: 0.0 1.5551376342773438\n"
     ]
    }
   ],
   "source": [
    "# Indexing, boolean masks, where\n",
    "M = torch.randn(4, 4)\n",
    "mask = M > 0\n",
    "pos = M[mask]\n",
    "M2 = torch.where(M > 0, M, torch.zeros_like(M))   # keep positives, zero else\n",
    "\n",
    "print(\"positives count:\", pos.numel())\n",
    "print(\"M2 nonneg min/max:\", M2.min().item(), M2.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors on GPU\n",
    "\n",
    "By default, tensors you create live on the **CPU**. To fully leverage PyTorch performance, you can move tensors (and models) to a **GPU** and perform operations there.\n",
    "\n",
    "Why GPUs? They’re optimized for massively parallel floating-point math, so many tensor ops (e.g., matrix multiplies, convolutions) run much faster than on CPUs.\n",
    "\n",
    "**How to use the GPU in PyTorch**\n",
    "- Pick a device: `device = torch.device(\"cuda\")` if `torch.cuda.is_available()` else `\"cpu\"`.\n",
    "- Move existing tensors: `x = x.to(device)` or `x = x.cuda()` / `x = x.cpu()`.\n",
    "- Create tensors directly on the target device: `torch.zeros((a, b), device=device)`.\n",
    "- Models must be on the same device as their inputs: `model.to(device); out = model(x.to(device))`.\n",
    "- Avoid frequent CPU↔GPU transfers inside loops; move once and keep data on the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "# 1) Check CUDA and pick a device\n",
    "import torch\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", cuda)\n",
    "if cuda:\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.device (before): cpu\n",
      "A.device (after): cuda:0\n",
      "B.device: cuda:0 | C.device: cuda:0\n",
      "Z.shape: torch.Size([3, 4]) | Z.device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 2) Move tensors to GPU (or create them there)\n",
    "\n",
    "# A CPU tensor\n",
    "A = torch.zeros((3, 4))               # lives on CPU by default\n",
    "print(\"A.device (before):\", A.device)\n",
    "\n",
    "# Move to target device\n",
    "A = A.to(device)\n",
    "print(\"A.device (after):\", A.device)\n",
    "\n",
    "# Create directly on device (preferred)\n",
    "B = torch.randn((3, 4), device=device)\n",
    "C = torch.ones_like(B)                 # inherits shape/dtype/device\n",
    "print(\"B.device:\", B.device, \"| C.device:\", C.device)\n",
    "\n",
    "# Do some ops on the chosen device\n",
    "Z = A + B + C\n",
    "print(\"Z.shape:\", Z.shape, \"| Z.device:\", Z.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU devices and indices (`cuda:0`, `cuda:1`, …)\n",
    "\n",
    "When a tensor’s device shows `cuda:0`, it means the tensor lives in **GPU memory (VRAM)** on GPU **ID 0**.  \n",
    "If you have multiple GPUs, you can place tensors on different devices: `cuda:0`, `cuda:1`, …, `cuda:N`.\n",
    "\n",
    "> **Note:** The numbering refers to *visible* GPUs and can be affected by the `CUDA_VISIBLE_DEVICES` environment variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 8\n",
      "GPU 0: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 2: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 3: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 4: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 5: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 6: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 7: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}:\", torch.cuda.get_device_name(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All tensors in an operation must be on the **same device**\n",
    "\n",
    "PyTorch won’t implicitly move tensors across devices for you.  \n",
    "If operands live on different devices (e.g., `cpu` and `cuda:0`), you’ll get an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mismatch error:\n",
      "  Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n",
      "OK after .to('cuda:0') → device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example mismatch\n",
    "cpu_t = torch.ones((2, 2))                       # CPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_t = torch.ones((2, 2), device=\"cuda:0\")  # GPU\n",
    "\n",
    "    # This will raise a device mismatch error\n",
    "    try:\n",
    "        cpu_t + gpu_t\n",
    "    except RuntimeError as e:\n",
    "        print(\"Device mismatch error:\\n \", e)\n",
    "\n",
    "    # Fix: move one tensor so both are on the same device\n",
    "    fixed = cpu_t.to(\"cuda:0\") + gpu_t\n",
    "    print(\"OK after .to('cuda:0') → device:\", fixed.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are GPU computations really faster?\n",
    "\n",
    "Often **yes** for large, parallelizable workloads (e.g., big matrix multiplies, convolutions).  \n",
    "However, for **small tensors** or **many tiny ops**, CPU can be similar or faster due to transfer/launch overheads.  \n",
    "Performance depends on tensor size, dtype (FP32/FP16/BF16/FP64), hardware, and how often you move data between CPU and GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does `torch.cuda.synchronize()` do?\n",
    "\n",
    "CUDA ops in PyTorch are **asynchronous**: they enqueue work on the GPU and return immediately.  \n",
    "`synchronize()` **waits** until all queued GPU work is finished. You typically **don’t need it** for correctness. PyTorch syncs implicitly at certain points (e.g., when copying a tensor to CPU with `.cpu()/.numpy()`, or calling `.item()` on a CUDA tensor).  \n",
    "Use `synchronize()` mainly for **accurate timing/benchmarking** or when you explicitly need a barrier between GPU tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU avg: 13.783ms\n",
      "GPU avg: 5.648ms\n"
     ]
    }
   ],
   "source": [
    "# Quick (optional) benchmark: CPU vs GPU matmul\n",
    "import torch, time\n",
    "\n",
    "def bench_matmul(n=2048, device=\"cpu\", iters=5):\n",
    "    x = torch.randn((n, n), device=device)\n",
    "    y = torch.randn((n, n), device=device)\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()  # ensure fair timing start\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        z = x @ y\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()  # ensure kernels finished\n",
    "    return (time.perf_counter() - t0) / iters\n",
    "\n",
    "size_matrix = 2048\n",
    "\n",
    "cpu_t = bench_matmul(n=size_matrix, device=\"cpu\", iters=10)\n",
    "print(f\"CPU avg: {cpu_t*1000:.3f}ms\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_t = bench_matmul(n=size_matrix, device=\"cuda:0\", iters=10)\n",
    "    print(f\"GPU avg: {gpu_t*1000:.3f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive timing: 0.3006681799888611 ms (likely underestimates)\n",
      "Synced timing: 3.11391893774271 ms (actual elapsed)\n"
     ]
    }
   ],
   "source": [
    "# Timing example with/without synchronize (optional)\n",
    "import torch, time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn((4096, 4096), device=\"cuda:0\")\n",
    "    y = torch.randn((4096, 4096), device=\"cuda:0\")\n",
    "\n",
    "    # Incorrect timing (may be too small because of async execution)\n",
    "    t0 = time.perf_counter()\n",
    "    z = x @ y\n",
    "    t1 = time.perf_counter()\n",
    "    print(\"Naive timing:\", (t1 - t0)*1000, \"ms (likely underestimates)\")\n",
    "\n",
    "    # Correct timing with explicit sync\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    z = x @ y\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    print(\"Synced timing:\", (t1 - t0)*1000, \"ms (actual elapsed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients, computation graphs, and backpropagation\n",
    "\n",
    "PyTorch includes **autograd**, a system that **records what you do** to tensors during the forward pass and then **automatically computes gradients** (derivatives) for you during the backward pass.\n",
    "\n",
    "### What are gradients?\n",
    "Gradients tell you **how much a change in a parameter changes the loss** (and in which direction). They are the “compass” that points updates the right way during training.\n",
    "\n",
    "### What is a computation graph?\n",
    "As you do operations (add, multiply, matmul, etc.), PyTorch builds a **graph** that connects inputs → outputs.  \n",
    "It’s **dynamic**: the graph is created on the fly each time you run your Python code, so `if/for/while` just work naturally.\n",
    "\n",
    "### What is backpropagation?\n",
    "After you compute a single **scalar** loss (a single number), calling `loss.backward()` walks **backwards** through that graph and fills in the `.grad` field of any **leaf tensors** that asked for gradients (`requires_grad=True`).  \n",
    "If your output isn’t a single number, you must provide an “upstream” gradient: `y.backward(grad)` where `grad` has the same shape as `y`.\n",
    "\n",
    "### How it works in practice\n",
    "1. Create parameters/tensors with `requires_grad=True`.  \n",
    "2. Do a **forward pass** to compute a **loss** (a single number).  \n",
    "3. Call `loss.backward()` to compute gradients.  \n",
    "4. Read/update parameters (e.g., with an optimizer) and **reset grads** before the next step.\n",
    "\n",
    "### Handy controls\n",
    "- **Turn off tracking** when you don’t need gradients (e.g., inference):  \n",
    "  `with torch.no_grad(): ...` or `x = x.detach()`\n",
    "- **Intermediate results** don’t keep `.grad` by default (only leaf tensors do).  \n",
    "  If you want to inspect them, call `t.retain_grad()` (optional/advanced).\n",
    "- **Gradients accumulate** across steps. Clear them with `param.grad = None` (preferred) or `param.grad.zero_()` each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad (should equal x): tensor([ 2., -1.,  3.])\n"
     ]
    }
   ],
   "source": [
    "# 1) Minimal autograd demo: scalar output → simple backward\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0, -1.0, 3.0], requires_grad=True)   # leaf tensor\n",
    "# f(x) = 1/2 * sum(x^2)  → df/dx = x\n",
    "y = 0.5 * (x**2).sum()\n",
    "y.backward()  # computes ∂y/∂x\n",
    "print(\"x.grad (should equal x):\", x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.grad shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 2) Non-scalar output: provide an upstream gradient to backward\n",
    "import torch\n",
    "\n",
    "A = torch.randn(2, 3, requires_grad=True)\n",
    "s = (A**2).sum(dim=1)                # shape: (2,)  ← non-scalar output\n",
    "upstream = torch.tensor([1.0, 0.5])  # weights for each component\n",
    "s.backward(upstream)                 # J^T · upstream\n",
    "print(\"A.grad shape:\", A.grad.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | grad norm: 12.824451446533203\n",
      "step 1 | grad norm: 12.824451446533203\n",
      "step 2 | grad norm: 12.824451446533203\n"
     ]
    }
   ],
   "source": [
    "# 3) Gradients accumulate; clear them between steps\n",
    "import torch\n",
    "\n",
    "w = torch.randn(3, 3, requires_grad=True)\n",
    "\n",
    "for step in range(3):\n",
    "    # Preferred clearing method: set to None (lets PyTorch allocate lazily next time)\n",
    "    w.grad = None\n",
    "\n",
    "    out = (w @ w.t()).sum()   # simple scalar \"loss\"\n",
    "    out.backward()\n",
    "    print(f\"step {step} | grad norm:\", w.grad.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.grad (kept via retain_grad): tensor([ -2.4775,  -3.9115,   2.4936, -11.3991])\n",
      "Requires grad? h: True | z(no_grad): False | z_detached: False\n"
     ]
    }
   ],
   "source": [
    "# 4) Detach / no_grad / retain_grad on intermediates\n",
    "import torch\n",
    "\n",
    "x = torch.randn(4, requires_grad=True)\n",
    "h = x * 3.0                   # non-leaf intermediate\n",
    "h.retain_grad()               # keep grad for inspection\n",
    "loss = (h**2).sum()\n",
    "loss.backward()\n",
    "print(\"h.grad (kept via retain_grad):\", h.grad)\n",
    "\n",
    "# Stop tracking:\n",
    "with torch.no_grad():\n",
    "    z = h + 1                 # no graph recording here\n",
    "\n",
    "z_detached = h.detach()        # also stops tracking\n",
    "print(\"Requires grad? h:\", h.requires_grad, \"| z(no_grad):\", z.requires_grad, \"| z_detached:\", z_detached.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad norm: 1.625781536102295\n"
     ]
    }
   ],
   "source": [
    "# Autograd: out-of-place vs in-place; no_grad for inference\n",
    "w = torch.randn(3, 3, requires_grad=True)\n",
    "inp = torch.randn(5, 3)\n",
    "out = inp @ w        # uses matmul; creates grad_fn on out\n",
    "loss = out.pow(2).mean()\n",
    "loss.backward()\n",
    "print(\"w.grad norm:\", w.grad.norm().item())\n",
    "\n",
    "with torch.no_grad():  # turn off grad tracking (e.g., inference)\n",
    "    pred = inp @ w\n",
    "    w.add_(0.0)        # in-place is fine here since autograd is disabled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling gradients (memory note)\n",
    "\n",
    "When you create a tensor with `requires_grad=True`, PyTorch starts **recording the operations** you apply to it so it can compute gradients later.  \n",
    "This tracking **uses extra memory** (to store intermediate results and graph metadata). After you’re done (e.g., during inference), you can avoid this overhead with:\n",
    "- `with torch.no_grad(): ...` or\n",
    "- `x = x.detach()` to stop tracking a tensor.\n",
    "\n",
    "> **Only floating or complex tensors can require gradients.** Integer tensors (`int32/int64`, etc.) cannot track gradients.\n",
    "\n",
    "Below, let’s do a simple **multiplication** and backpropagate to see the gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise example: \\(F = \\sqrt{x^2 + y^2}\\)\n",
    "\n",
    "- `grad_fn` shows the **operation that produced a tensor** (the last node in the graph for that tensor).\n",
    "  - For a scalar multiplication like `B = 3.0 * A`, you’ll typically see **`MulBackward0`**.\n",
    "  - For our \\($F=\\sqrt{x^2+y^2}$\\) example, you’ll typically see **`SqrtBackward0`**.\n",
    "- To **get gradients**, call `.backward()` on a **scalar**:\n",
    "  - Reduce vector outputs to a scalar (e.g., `F.sum().backward()`), **or**\n",
    "  - Provide an upstream gradient: `F.backward(torch.ones_like(F))`.\n",
    "\n",
    "Analytic partial derivatives:\n",
    "\\[\n",
    "$\\frac{\\partial F}{\\partial x}=\\frac{x}{\\sqrt{x^2+y^2}}$,$\\qquad$\n",
    "$\\frac{\\partial F}{\\partial y}=\\frac{y}{\\sqrt{x^2+y^2}}$.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: tensor([2.0616, 4.1231, 3.6056], grad_fn=<SqrtBackward0>)\n",
      "F.grad_fn: SqrtBackward0\n",
      "B.grad_fn (scalar multiply): MulBackward0\n",
      "\n",
      "x.grad: tensor([ 0.9701, -0.2425,  0.8321])\n",
      "y.grad: tensor([ 0.2425,  0.9701, -0.5547])\n",
      "\n",
      "Matches analytic ∂F/∂x? True\n",
      "Matches analytic ∂F/∂y? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Two small float tensors that require gradients\n",
    "x = torch.tensor([ 2.0, -1.0,  3.0], requires_grad=True)\n",
    "y = torch.tensor([ 0.5,  4.0, -2.0], requires_grad=True)\n",
    "\n",
    "# F = sqrt(x^2 + y^2)  (elementwise)\n",
    "F = torch.sqrt(x**2 + y**2)\n",
    "print(\"F:\", F)\n",
    "print(\"F.grad_fn:\", type(F.grad_fn).__name__)   # typically 'SqrtBackward0'\n",
    "\n",
    "# (Optional) show grad_fn for a scalar multiplication\n",
    "B = 3.0 * x\n",
    "print(\"B.grad_fn (scalar multiply):\", type(B.grad_fn).__name__)  # typically 'MulBackward0'\n",
    "\n",
    "# Backprop: reduce to a scalar loss, then backward\n",
    "loss = F.sum()          # scalar\n",
    "loss.backward()         # computes d(loss)/dx and d(loss)/dy\n",
    "\n",
    "print(\"\\nx.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)\n",
    "\n",
    "# Check against analytic gradients: x/F and y/F\n",
    "eps = 1e-12\n",
    "x_analytic = x.detach() / (F.detach() + eps)\n",
    "y_analytic = y.detach() / (F.detach() + eps)\n",
    "\n",
    "print(\"\\nMatches analytic ∂F/∂x?\", torch.allclose(x.grad, x_analytic, rtol=1e-5, atol=1e-7))\n",
    "print(\"Matches analytic ∂F/∂y?\", torch.allclose(y.grad, y_analytic, rtol=1e-5, atol=1e-7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: tensor([2.0616, 4.1231, 3.6056], grad_fn=<SqrtBackward0>)\n",
      "Autodiff gradients\n",
      "x.grad: tensor([ 0.9701, -0.2425,  0.8321])\n",
      "y.grad: tensor([ 0.2425,  0.9701, -0.5547])\n",
      "\n",
      "Comparison with analytic gradients\n",
      "x / sqrt(x^2 + y^2): tensor([ 0.9701, -0.2425,  0.8321])\n",
      "y / sqrt(x^2 + y^2): tensor([ 0.2425,  0.9701, -0.5547])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Fresh tensors (so grads start as None)\n",
    "x = torch.tensor([ 2.0, -1.0,  3.0], requires_grad=True)\n",
    "y = torch.tensor([ 0.5,  4.0, -2.0], requires_grad=True)\n",
    "\n",
    "F = torch.sqrt(x**2 + y**2)\n",
    "print(\"F:\", F)\n",
    "\n",
    "# Option A (recommended): reduce to a scalar, then backward\n",
    "F.sum().backward()\n",
    "\n",
    "print(\"Autodiff gradients\")\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)\n",
    "\n",
    "print(\"\\nComparison with analytic gradients\")\n",
    "den = torch.sqrt(x.detach()**2 + y.detach()**2) + 1e-12\n",
    "print(\"x / sqrt(x^2 + y^2):\", x.detach() / den)\n",
    "print(\"y / sqrt(x^2 + y^2):\", y.detach() / den)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodiff gradients (with explicit upstream):\n",
      "x.grad: tensor([ 0.9701, -0.2425,  0.8321])\n",
      "y.grad: tensor([ 0.2425,  0.9701, -0.5547])\n"
     ]
    }
   ],
   "source": [
    "# Reset grads and do backward with an explicit upstream gradient\n",
    "x.grad = None\n",
    "y.grad = None\n",
    "\n",
    "F = torch.sqrt(x**2 + y**2)\n",
    "F.backward(torch.ones_like(F))  # provide grad for non-scalar output\n",
    "\n",
    "print(\"Autodiff gradients (with explicit upstream):\")\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detaching tensors (to stop tracking and save memory)\n",
    "\n",
    "`tensor.detach()` returns a **view** of the same data **without** gradient tracking.  \n",
    "Use it when you don’t need autograd for downstream ops (e.g., logging, metrics, visualization) to **avoid building bigger graphs and using extra memory**.\n",
    "\n",
    "- `z = x.detach()` → stops gradient flow from `z` back into `x`.\n",
    "- `z = x.detach().clone()` → makes a **separate copy** that’s also not tracked (useful if you need an independent buffer).\n",
    "- `with torch.no_grad(): ...` → context to run a whole block without tracking (great for **inference**).\n",
    "- `x.requires_grad_(False)` → flips the flag on `x` itself (be careful if `x` is a learnable parameter).\n",
    "\n",
    "Typical use when converting to NumPy for plotting/logging:\n",
    "```python\n",
    "value_np = tensor.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad → x: True | h: True | z: False\n",
      "x.grad: tensor([18., 36., 54.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "h = x * 3.0                       # tracked intermediate\n",
    "z = h.detach()                    # NOT tracked\n",
    "\n",
    "print(\"requires_grad → x:\", x.requires_grad, \"| h:\", h.requires_grad, \"| z:\", z.requires_grad)\n",
    "\n",
    "# Only x/h are part of the graph\n",
    "loss = (h**2).sum()\n",
    "loss.backward()\n",
    "print(\"x.grad:\", x.grad)          # has gradients\n",
    "# z has no grad and isn't connected to the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up & what’s next\n",
    "\n",
    "In practice, most training loops end with a **scalar loss**, so `loss.backward()` (no arguments) is all you need.  \n",
    "If you have **multi-output** objectives, either **reduce** them to one scalar (e.g., sum/mean or a weighted combo), or supply an **upstream gradient** with the same shape as the output: `y.backward(grad)`.\n",
    "\n",
    "**Next tutorial preview — training a tiny neural network**\n",
    "\n",
    "We’ll put this into practice by training a small model end-to-end:\n",
    "1) Define a dataset and a tiny network.  \n",
    "2) Forward pass to get predictions.  \n",
    "3) Compute a **scalar loss** (e.g., MSE or cross-entropy).  \n",
    "4) Backward pass: `loss.backward()`.  \n",
    "5) Update parameters: `optimizer.step()`.  \n",
    "6) Reset gradients: `optimizer.zero_grad(set_to_none=True)`.\n",
    "\n",
    "We’ll also cover keeping model and tensors on the **same device** (CPU/GPU), toggling `model.train()` / `model.eval()`, and using `with torch.no_grad()` during evaluation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
